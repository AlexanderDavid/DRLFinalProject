{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "random-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "demanding-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, width, height, std, device=\"cpu\"):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.depth_conv = nn.Sequential(\n",
    "            nn.Conv3d(1, 1, (1, 4, 4), (1, 2, 2), padding=(0, 1, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv3d(1, 64, (10, 1, 1), (1, 1, 1), padding=(0, 0, 0)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool3d((1, 2, 2)),\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(20480)\n",
    "        ).to(device)\n",
    "        self.depth_conv.apply(ActorCritic.__init_weights)\n",
    "        \n",
    "\n",
    "        self.action_prediction = nn.Sequential(\n",
    "            nn.Linear(20484, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 2),\n",
    "        ).to(device)\n",
    "\n",
    "        self.state_prediction = nn.Sequential(\n",
    "            nn.Linear(20484, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 1),\n",
    "        ).to(device)\n",
    "\n",
    "        self.cov_mat = torch.eye(2) * (std ** 2)\n",
    "        self.action_var = torch.full((2,), std**2).to(device)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    @staticmethod\n",
    "    def __init_weights(m):\n",
    "        if type(m) == nn.Conv3d:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, depth, goal, vel, greedy=False):\n",
    "        # Translate everything to tensors of the correct shape\n",
    "        if type(depth) is not torch.Tensor:\n",
    "            depth = torch.Tensor(list(depth)).view(-1, 1, 10, 64, 80)\n",
    "        if type(goal) is not torch.Tensor:\n",
    "            goal = torch.Tensor(goal).view(-1, 2)\n",
    "        if type(vel) is not torch.Tensor:\n",
    "            vel = torch.Tensor(vel).view(-1, 2)\n",
    "\n",
    "        # Convolve the depth image stack and concat with the goal and last velocity\n",
    "        conv = self.depth_conv(depth)\n",
    "        catted = torch.cat((conv, goal, vel), dim=1)\n",
    "\n",
    "        # Get the means for the two actions\n",
    "        action_means = self.action_prediction(catted).view(-1, 2)\n",
    "\n",
    "        # Sample from a normal distribution for each agent in the batch\n",
    "        linears = []\n",
    "        rotations = []\n",
    "        logprobs = []\n",
    "        for action_mean in action_means:\n",
    "            if greedy:\n",
    "                sample = action_mean\n",
    "                logprobs.append(None)\n",
    "            else:\n",
    "                dist = MultivariateNormal(action_mean, self.cov_mat)\n",
    "                sample = dist.sample()\n",
    "                logprobs.append(dist.log_prob(sample))\n",
    "\n",
    "            linears.append(torch.sigmoid(sample[0]))\n",
    "            rotations.append(torch.tanh(sample[1]))\n",
    "\n",
    "        # Return the zipped actions\n",
    "        return zip(linears, rotations, logprobs)\n",
    "\n",
    "    def evaluate(self, depth, goal, vel, action):\n",
    "        # Translate everything to tensors of the correct shape\n",
    "        if type(depth) is not torch.Tensor:\n",
    "            depth = torch.Tensor(list(depth)).view(-1, 1, 10, 64, 80)\n",
    "        if type(goal) is not torch.Tensor:\n",
    "            goal = torch.Tensor(goal).view(-1, 2)\n",
    "        if type(vel) is not torch.Tensor:\n",
    "            vel = torch.Tensor(vel).view(-1, 2)\n",
    "\n",
    "        # Convolve the depth image stack and concat with the goal and last velocity\n",
    "        torch.save(depth, \"last_depth.pt\")\n",
    "        conv = self.depth_conv(depth)\n",
    "        catted = torch.cat((conv, goal, vel), dim=1)\n",
    "\n",
    "        # Get the means for the two actions\n",
    "        action_means = self.action_prediction(catted).view(-1, 2)\n",
    "\n",
    "        action_var = self.action_var.expand_as(action_means)\n",
    "        cov_mat = torch.diag_embed(action_var).to(self.device)\n",
    "        dist = MultivariateNormal(action_means, cov_mat)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.state_prediction(catted)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "403e7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, width, height, action_std, lr=0.0003, gamma=0.99, K_epochs=5, eps_clip=0.2, device=\"cpu\"):\n",
    "        super(PPO, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.device = device\n",
    "\n",
    "        self.policy = ActorCritic(width, height, action_std).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        self.policy_old = ActorCritic(width, height, action_std).to(self.device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, depth, goal, vel, greedy=False):\n",
    "        return self.policy_old(depth, goal, vel, greedy)\n",
    "\n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_depths = torch.squeeze(torch.stack([state[0] for state in memory.states]).to(self.device), 1).detach().view(-1, 1, 10, 64, 80)\n",
    "        old_goals = torch.squeeze(torch.stack([state[1] for state in memory.states]).to(self.device), 1).detach().view(-1, 2)\n",
    "        old_vels = torch.squeeze(torch.stack([state[2] for state in memory.states]).to(self.device), 1).detach().view(-1, 2)\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions).to(self.device), 1).detach().view(-1, 2)\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(self.device).detach().view(-1, 1)\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        losses = []\n",
    "        print(\"--- Starting to Train ---\")\n",
    "        for epoch in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            try:\n",
    "                logprobs, state_values, dist_entropy = self.policy.evaluate(old_depths, old_goals, old_vels, old_actions)\n",
    "            except:\n",
    "                print(\"Skipping epoch due to conv NaN\")\n",
    "                continue\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            losses.append(loss.mean().item() / self.K_epochs)\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            print(epoch, \" -- \", self.policy.depth_conv[0].weight.mean())\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6e747cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/share/virtualenvs/comp_viz_drl_final-Nj5FmqyT/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import depth_collision_avoidance_env\n",
    "import gym\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "\n",
    "# creating environment\n",
    "env_name = \"DepthCollisionAvoidance-v0\"\n",
    "env = gym.make(env_name)\n",
    "ppo = PPO(64, 80, 0.001)\n",
    "memory = Memory()\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "t = 0\n",
    "episodes = 500\n",
    "max_len = 100\n",
    "\n",
    "rewards = []\n",
    "lengths = []\n",
    "losses = []\n",
    "running_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f04d5ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "--- Starting to Train ---\n",
      "0  --  tensor(-0.0064, grad_fn=<MeanBackward0>)\n",
      "1  --  tensor(-0.0061, grad_fn=<MeanBackward0>)\n",
      "2  --  tensor(-0.0060, grad_fn=<MeanBackward0>)\n",
      "3  --  tensor(-0.0058, grad_fn=<MeanBackward0>)\n",
      "4  --  tensor(-0.0057, grad_fn=<MeanBackward0>)\n",
      "--- Loss 0.9216342568397522 ---\n",
      "Last episode reward: -0.9415604811561559\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "--- Starting to Train ---\n",
      "0  --  tensor(-0.0056, grad_fn=<MeanBackward0>)\n",
      "1  --  tensor(-0.0055, grad_fn=<MeanBackward0>)\n",
      "2  --  tensor(nan, grad_fn=<MeanBackward0>)\n",
      "Skipping epoch due to conv NaN\n",
      "Skipping epoch due to conv NaN\n",
      "--- Loss nan ---\n",
      "Last episode reward: -0.0062799664432365565\n",
      "tensor(False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The parameter loc has invalid values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0217b9542856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mparsed_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/comp_viz_drl_final-Nj5FmqyT/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-db8b3a1483d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, depth, goal, vel, greedy)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/comp_viz_drl_final-Nj5FmqyT/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-abfd0c92cc52>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, depth, goal, vel, greedy)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mlogprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mlogprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/comp_viz_drl_final-Nj5FmqyT/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale_tril\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/comp_viz_drl_final-Nj5FmqyT/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter loc has invalid values"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    for i in range(max_len):\n",
    "        t += 1\n",
    "        # Convert observations to batches of tensors\n",
    "        depths = torch.stack([torch.Tensor(list(obs[agent_id][0])) for agent_id in obs]).view(-1, 1, 10, 64, 80)\n",
    "        goals = torch.stack([torch.Tensor(list(obs[agent_id][1])) for agent_id in obs])\n",
    "        vels = torch.stack([torch.Tensor(list(obs[agent_id][2])) for agent_id in obs])\n",
    "        depths /= 4.5\n",
    "        depths -= 0.5\n",
    "        print(depths.isnan().sum() > 1)\n",
    "\n",
    "        actions = list(ppo(depths, goals, vels))\n",
    "\n",
    "        parsed_actions = {agent_id: [a.cpu().numpy() for a in actions[i][:2]] for i, agent_id in enumerate(obs)}\n",
    "        obs, r, done, _ = env.step(parsed_actions, collisions=False)\n",
    "        running_reward += sum([r[id] for id in r])\n",
    "\n",
    "        for action, depth, goal, vel, reward in zip(actions, depths, goals, vels, r):\n",
    "            memory.actions.append(torch.Tensor(action[:2]))\n",
    "            memory.states.append((depth.view(10, 64, 80), goal, vel))\n",
    "            memory.logprobs.append(action[2])\n",
    "            memory.rewards.append(r[reward])\n",
    "            memory.is_terminals.append(done)\n",
    "\n",
    "        if len(memory.rewards) % 400 == 0:\n",
    "            # For some reason the Conv layer is producing NaN sometimes which will cause\n",
    "            # training to fail. Until this is solved just skip that training step if this\n",
    "            # happens\n",
    "            try:\n",
    "                loss = ppo.update(memory)\n",
    "                print(f\"--- Loss {loss} ---\")\n",
    "                losses.append((t, loss))\n",
    "            except:\n",
    "                print(f\"Failed to train at step {t}\")\n",
    "                pass\n",
    "            \n",
    "            memory.clear_memory()\n",
    "\n",
    "    rewards.append((t, running_reward))\n",
    "    lengths.append((t, i))\n",
    "    running_reward = 0\n",
    "\n",
    "    print(f\"Last episode reward: {rewards[-1][1]}\")\n",
    "\n",
    "    if episode % 50 == 49:\n",
    "        print(f\"[{episode:03}/{episodes}\\tAvg Reward: {mean([r[1] for r in rewards[-50:]])}\\tAvg Lengths: {mean([l[1] for l in lengths[-50:]])}\")\n",
    "        torch.save(ppo.policy.state_dict(), \"./PPO_checkpoint.pt\")\n",
    "        np.savetxt(f\"./playbacks/{episode:03}.csv\", env.playback)\n",
    "        pickle.dump((rewards, lengths, losses), open(\"./results_checkpoint.pickle\", \"wb+\"))\n",
    "\n",
    "    running_reward = 0\n",
    "\n",
    "torch.save(ppo.policy.state_dict(), \"./PPO.pt\")\n",
    "pickle.dump((rewards, lengths, losses), open(\"./results.pickle\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b5878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
